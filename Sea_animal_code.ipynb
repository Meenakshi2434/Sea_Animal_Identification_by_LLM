{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzwJ/5qBICt052lef54h0a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Meenakshi2434/Sea_Animal_Identification_by_LLM/blob/main/Sea_animal_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8nGmvovMdgM"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "from PIL import Image\n",
        "import io\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY = \"ENTER_YOUR_API\"\n",
        "genai.configure(api_key=API_KEY)"
      ],
      "metadata": {
        "id": "CfflZouKPA44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use \"gemini-1.5-flash-latest\" for the most recent Flash model\n",
        "MODEL_NAME = \"gemini-2.0-flash\"\n",
        "\n",
        "# --- Image Loading Function ---\n",
        "def load_image_from_path(image_path):\n",
        "    \"\"\"Loads an image from a given file path and returns it as a PIL Image.\"\"\"\n",
        "    try:\n",
        "        with Image.open(image_path) as img:\n",
        "            # It's good practice to convert to RGB to ensure compatibility with the API\n",
        "            return img.convert(\"RGB\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Image file not found at '{image_path}'.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading image '{image_path}': {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "madPgfn1PPvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANT: Replace this with the actual path to your image file\n",
        "image_path = \"/content/IMG-20250505-WA0092.jpg\""
      ],
      "metadata": {
        "id": "Gp7j4fn2PXh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_image = load_image_from_path(image_path)"
      ],
      "metadata": {
        "id": "T502kZMaPmXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if input_image:\n",
        "    # Initialize the Generative Model\n",
        "    model = genai.GenerativeModel(MODEL_NAME)\n",
        "\n",
        "    # Define the prompt content\n",
        "    # The image is included directly in the content list\n",
        "    prompt_content = [\n",
        "                    input_image,\n",
        "                    \"Identify the marine species in this image. Provide 2 distinct predictions in total. \"\n",
        "                    \"For both predictions, provide the following details:\\n\"\n",
        "                    \"- 'common_name': (string)\\n\"\n",
        "                    \"- 'scientific_name': (string)\\n\"\n",
        "                    \"- 'family': (string)\\n\"\n",
        "                    \"- 'lifespan': (string, range in months or years)\\n\"\n",
        "                    \"- 'diet': (string)\\n\"\n",
        "                    \"- 'shape': (string)\\n\"\n",
        "                    \"- 'habitat': (string)\\n\"\n",
        "                    \"- 'size': (string, provide size with unit like 'Up to 2cm', 'About 1.5m')\\n\"\n",
        "                    \"- 'color': (array of strings)\\n\"\n",
        "                    \"- 'short_description': (string, minimum 40 words, rich and detailed description of the species' physical features, behaviors, and unique characteristics)\\n\"\n",
        "                    \"- 'locations_found_in': (array of strings; list major oceans, marine regions, countries, or climatic zones where the species is found)\\n\"\n",
        "                    \"Your response MUST be a valid JSON array, using double quotes only around both keys and string values. DO NOT use single quotes. DO NOT wrap the JSON inside any markdown (like ```json). DO NOT include any explanation or formatting.\"\n",
        "                ]\n",
        "\n",
        "    print(f\"Sending request to model: {MODEL_NAME}...\")\n",
        "    try:\n",
        "        # Generate content from the model\n",
        "        response = model.generate_content(prompt_content)\n",
        "\n",
        "        # Print the model's response\n",
        "        print(\"\\n--- Model Response ---\")\n",
        "        print(f\"[{response.text}]\")\n",
        "        print(\"---------------------\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred during content generation: {e}\")\n",
        "        print(\"Please check your API key, model name, and internet connection.\")\n",
        "        print(\"Also, ensure the image content is suitable for the model's safety filters.\")\n",
        "else:\n",
        "        print(\"Could not proceed without a valid image.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "U7ebvwaGPIwm",
        "outputId": "9f250a99-2446-46fd-88fb-a5191b7a3708"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sending request to model: gemini-2.0-flash...\n",
            "\n",
            "--- Model Response ---\n",
            "[```json\n",
            "[\n",
            "  {\n",
            "    \"common_name\": \"Weedy Scorpionfish\",\n",
            "    \"scientific_name\": \"Rhinopias frondosa\",\n",
            "    \"family\": \"Scorpaenidae\",\n",
            "    \"lifespan\": \"5-8 years\",\n",
            "    \"diet\": \"Carnivorous, feeds on small fish and crustaceans.\",\n",
            "    \"shape\": \"Elongated, compressed body with irregular appendages.\",\n",
            "    \"habitat\": \"Rocky reefs, coral rubble, and seagrass beds.\",\n",
            "    \"size\": \"Up to 23 cm\",\n",
            "    \"color\": [\n",
            "      \"Red\",\n",
            "      \"Brown\",\n",
            "      \"Pink\",\n",
            "      \"White\"\n",
            "    ],\n",
            "    \"short_description\": \"The Weedy Scorpionfish, *Rhinopias frondosa*, is a master of disguise, exhibiting remarkable camouflage to blend seamlessly with its surroundings. Its body is adorned with numerous leaf-like appendages, giving it a weedy appearance. These elaborate structures, combined with its coloration, allow it to mimic algae or seaweed. This species is a carnivore, patiently ambushing small fish and crustaceans. When threatened, it can intensify its coloration to further enhance its camouflage. They are solitary creatures and spend most of their time motionless, waiting for prey. This highly sought after species is a true wonder of the underwater world.\",\n",
            "    \"locations_found_in\": [\n",
            "      \"Indo-Pacific Ocean\",\n",
            "      \"Indonesia\",\n",
            "      \"Philippines\",\n",
            "      \"Australia\",\n",
            "      \"Japan\"\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"common_name\": \"Lacy Scorpionfish\",\n",
            "    \"scientific_name\": \"Rhinopias aphanes\",\n",
            "    \"family\": \"Scorpaenidae\",\n",
            "    \"lifespan\": \"5-8 years\",\n",
            "    \"diet\": \"Carnivorous, preys on small fishes and crustaceans.\",\n",
            "    \"shape\": \"Compressed body with ornate, lacy appendages.\",\n",
            "    \"habitat\": \"Coral reefs, rocky areas and rubble zones.\",\n",
            "    \"size\": \"Up to 25 cm\",\n",
            "    \"color\": [\n",
            "      \"Red\",\n",
            "      \"Pink\",\n",
            "      \"Yellow\",\n",
            "      \"White\"\n",
            "    ],\n",
            "    \"short_description\": \"The Lacy Scorpionfish (*Rhinopias aphanes*) is celebrated for its extraordinary camouflage. Its body features elaborate, leaf-like appendages that resemble algae or seaweed, enabling it to blend perfectly with coral reefs and rocky habitats. This species is a predatory ambush hunter that patiently waits for unsuspecting prey to come within striking distance. Lacy Scorpionfish are usually solitary and display a range of colors to match their surroundings. Their ability to mimic their habitat makes them difficult to spot. The lacy scorpionfish is a sought after subject for underwater photography.\",\n",
            "    \"locations_found_in\": [\n",
            "      \"Indo-Pacific Ocean\",\n",
            "      \"Great Barrier Reef\",\n",
            "      \"Papua New Guinea\",\n",
            "      \"Indonesia\"\n",
            "    ]\n",
            "  }\n",
            "]\n",
            "```]\n",
            "---------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parsing of json response of llm.\n",
        "# Attempt to parse the JSON response\n",
        "import re\n",
        "# Use a regular expression to find the JSON array string within a ```json ... ``` block\n",
        "match = re.search(r\"```json\\s*(.*?)\\s*```\", response.text, re.DOTALL)  # removed response.text\n",
        "\n",
        "json_string = \"\"\n",
        "if match:\n",
        "    json_string = match.group(1).strip() # Extract the content inside the block\n",
        "else:\n",
        "    # Fallback: if no markdown block, try to strip and parse directly\n",
        "    json_string = response.text.strip()\n",
        "\n",
        "if not json_string:\n",
        "    raise ValueError(\"Extracted JSON string is empty or invalid.\")\n",
        "\n",
        "# Ensure the JSON string starts and ends with [] for an array\n",
        "if not (json_string.startswith('[') and json_string.endswith(']')):\n",
        "    print(\"Warning: Model did not return a top-level JSON array. Attempting to force array format.\")\n",
        "    # Attempt to wrap it in an array if it returned a single object\n",
        "    if json_string.startswith('{') and json_string.endswith('}'):\n",
        "        json_string = f\"[{json_string}]\"\n",
        "    else:\n",
        "        raise ValueError(\"Model did not return a valid JSON array or single object.\")\n",
        "\n",
        "parsed_responses = json.loads(json_string)\n",
        "\n"
      ],
      "metadata": {
        "id": "HoZj2Q98RHJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "J-7QUy9iRcAW",
        "outputId": "8c0b342b-64c1-49f4-f4aa-ed427bd1224d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (4.13.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.4.26)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (4.13.2)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=136fce480656b2b85c21a89445b6e9ca51131e5011354c712975b5fe630e7725\n",
            "  Stored in directory: /root/.cache/pip/wheels/8f/ab/cb/45ccc40522d3a1c41e1d2ad53b8f33a62f394011ec38cd71c6\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import wikipedia\n",
        "from urllib.parse import quote"
      ],
      "metadata": {
        "id": "wjErvvl3RZ-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# -------------------------\n",
        "# 1. Wikipedia API\n",
        "# -------------------------\n",
        "def fetch_wikipedia_image(query):\n",
        "    try:\n",
        "        page = wikipedia.page(query)\n",
        "        for img_url in page.images:\n",
        "            if img_url.lower().endswith(('.jpg', '.jpeg', '.png')) and 'logo' not in img_url.lower():\n",
        "                return img_url\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# -------------------------\n",
        "# 2. Wikimedia Commons Search API\n",
        "# -------------------------\n",
        "def fetch_commons_image(query):\n",
        "    try:\n",
        "        api_url = \"https://commons.wikimedia.org/w/api.php\"\n",
        "        params = {\n",
        "            \"action\": \"query\",\n",
        "            \"format\": \"json\",\n",
        "            \"prop\": \"imageinfo\",\n",
        "            \"generator\": \"search\",\n",
        "            \"gsrsearch\": query,\n",
        "            \"gsrlimit\": 5,\n",
        "            \"iiprop\": \"url\"\n",
        "        }\n",
        "        response = requests.get(api_url, params=params).json()\n",
        "        pages = response.get(\"query\", {}).get(\"pages\", {})\n",
        "        for page in pages.values():\n",
        "            imageinfo = page.get(\"imageinfo\", [])\n",
        "            if imageinfo:\n",
        "                url = imageinfo[0].get(\"url\", \"\")\n",
        "                if url.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                    return url\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# -------------------------\n",
        "# 3. FishBase (HTML Scrape Shortcut)\n",
        "# -------------------------\n",
        "def fetch_fishbase_image(scientific_name):\n",
        "    try:\n",
        "        fishbase_url = f\"https://www.fishbase.se/summary/{scientific_name.replace(' ', '-')}.html\"\n",
        "        response = requests.get(fishbase_url, timeout=5)\n",
        "        if response.ok and \"Pictures\" in response.text:\n",
        "            from bs4 import BeautifulSoup\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            img_tags = soup.select('img[src*=\"photos\"]')\n",
        "            for img in img_tags:\n",
        "                src = img.get(\"src\")\n",
        "                if src and not src.startswith(\"data:\"):\n",
        "                    return f\"https://www.fishbase.se{src}\"\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# -------------------------\n",
        "# 4. iNaturalist API\n",
        "# -------------------------\n",
        "def fetch_inaturalist_image(scientific_name):\n",
        "    try:\n",
        "        url = f\"https://api.inaturalist.org/v1/search?q={quote(scientific_name)}&sources=taxa\"\n",
        "        response = requests.get(url).json()\n",
        "        results = response.get(\"results\", [])\n",
        "        for r in results:\n",
        "            taxon = r.get(\"record\", {})\n",
        "            default_photo = taxon.get(\"default_photo\", {})\n",
        "            img_url = default_photo.get(\"medium_url\") or default_photo.get(\"url\")\n",
        "            if img_url:\n",
        "                return img_url\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# -------------------------\n",
        "# Main fetch function\n",
        "# -------------------------\n",
        "def fetch_images_for_prediction(common_name, scientific_name, max_images=3):\n",
        "    sources = [\n",
        "        lambda: fetch_wikipedia_image(scientific_name),\n",
        "        lambda: fetch_commons_image(scientific_name),\n",
        "        lambda: fetch_fishbase_image(scientific_name),\n",
        "        lambda: fetch_inaturalist_image(scientific_name),\n",
        "        lambda: fetch_wikipedia_image(common_name),\n",
        "        lambda: fetch_commons_image(common_name),\n",
        "        lambda: fetch_fishbase_image(common_name),\n",
        "        lambda: fetch_inaturalist_image(common_name),\n",
        "    ]\n",
        "\n",
        "    seen = set()\n",
        "    images = []\n",
        "    for fetch in sources:\n",
        "        if len(images) >= max_images:\n",
        "            break\n",
        "        try:\n",
        "            url = fetch()\n",
        "            if url and url not in seen:\n",
        "                seen.add(url)\n",
        "                images.append(url)\n",
        "        except Exception:\n",
        "            continue\n",
        "    return images\n"
      ],
      "metadata": {
        "id": "bws3QmSLRWwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for only image generation testing\n",
        "\n",
        "for i, prediction in enumerate(parsed_responses):\n",
        "    common_name = prediction.get(\"common_name\", \"\")\n",
        "    scientific_name = prediction.get(\"scientific_name\", \"\")\n",
        "\n",
        "    print(f\"\\nPrediction {i+1}: {common_name} ({scientific_name})\")\n",
        "\n",
        "    image_urls = fetch_images_for_prediction(common_name, scientific_name, max_images=3)\n",
        "\n",
        "    if image_urls:\n",
        "        for j, url in enumerate(image_urls, 1):\n",
        "            print(f\"  Image {j}: {url}\")\n",
        "    else:\n",
        "        print(\"  No images found from any source.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7xfgn6PRoZa",
        "outputId": "4832edae-b265-424a-cdcf-9adbdc1c7d60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prediction 1: Weedy Scorpionfish (Rhinopias frondosa)\n",
            "  Image 1: https://upload.wikimedia.org/wikipedia/commons/3/3d/Antennarius_striatus.jpg\n",
            "  Image 2: https://static.inaturalist.org/photos/29832944/medium.jpg\n",
            "  Image 3: https://inaturalist-open-data.s3.amazonaws.com/photos/72392466/medium.jpeg\n",
            "\n",
            "Prediction 2: Lacy Scorpionfish (Rhinopias aphanes)\n",
            "  Image 1: https://upload.wikimedia.org/wikipedia/commons/7/7b/Rhinopias_aphanes.jpg\n",
            "  Image 2: https://inaturalist-open-data.s3.amazonaws.com/photos/101692352/medium.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import folium\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import mapping, box, MultiPolygon"
      ],
      "metadata": {
        "id": "reN0qZ-3R6sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_location_name(name):\n",
        "    name = name.strip().lower().replace(\"-\", \" \").replace(\"_\", \" \")\n",
        "    # Remove common suffixes like \" region\", \" coast\"\n",
        "    name = re.sub(r\"\\b(region|coast|area|zone)\\b\", \"\", name)\n",
        "    return \" \".join(name.split())  # Normalize spaces"
      ],
      "metadata": {
        "id": "AxE3gmdmSsQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import new_regions"
      ],
      "metadata": {
        "id": "57B1kj14WO3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the above bounds into shapely polygons for visualization\n",
        "marine_zones_polygons = {}\n",
        "for region, info in new_regions.items():\n",
        "    norm_region = normalize_location_name(region)\n",
        "    bounds = info[\"bounds\"]\n",
        "    if isinstance(bounds, list):\n",
        "        polygons = [box(*b) for b in bounds]\n",
        "        marine_zones_polygons[norm_region] = MultiPolygon(polygons)\n",
        "    else:\n",
        "        marine_zones_polygons[norm_region] = box(*bounds)\n"
      ],
      "metadata": {
        "id": "946XiT2pS7pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install plotly geopy pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEtauvrPj1vZ",
        "outputId": "9a12371e-bb7e-4aa0-ed4b-3395c29c8d15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: geopy in /usr/local/lib/python3.11/dist-packages (2.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (9.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (24.2)\n",
            "Requirement already satisfied: geographiclib<3,>=1.52 in /usr/local/lib/python3.11/dist-packages (from geopy) (2.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from new_regions import new_regions, location_mapping\n"
      ],
      "metadata": {
        "id": "sGM8R772sZBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rapidfuzz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVImTudEXy1U",
        "outputId": "c647e72a-5897-4240-ac6e-404d63c2c1fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rapidfuzz\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz\n",
            "Successfully installed rapidfuzz-3.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rapidfuzz import process\n",
        "\n",
        "def fuzzy_match(loc_norm, choices, threshold=90):\n",
        "    match, score, _ = process.extractOne(loc_norm, choices)\n",
        "    return match if score >= threshold else None"
      ],
      "metadata": {
        "id": "1wOvcseZX3g-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_canonical_name(location):\n",
        "    norm_loc = normalize_location_name(location)\n",
        "    return location_mapping.get(norm_loc, norm_loc)  # Return mapped name or original\n",
        "\n",
        "import folium\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import mapping\n",
        "\n",
        "def create_distribution_map(locations, scientific_name, map_filename):\n",
        "    shapefile_path = \"/content/EEZ/EEZ_land_union_v4_202410/EEZ_land_union_v4_202410.shp\"\n",
        "    eez_gdf = gpd.read_file(shapefile_path)\n",
        "\n",
        "    # Normalize EEZ shapefile relevant columns to lowercase for matching\n",
        "    #cols_to_check = [\"TERRITORY1\", \"TERRITORY2\", \"TERRITORY3\", \"SOVEREIGN1\", \"SOVEREIGN2\", \"SOVEREIGN3\"]\n",
        "    #for col in cols_to_check:\n",
        "    #    if col in eez_gdf.columns:\n",
        "    #        eez_gdf[col] = eez_gdf[col].str.lower().fillna(\"\")\n",
        "\n",
        "    m = folium.Map(location=[0, 0], zoom_start=2, tiles=\"cartodbpositron\")\n",
        "\n",
        "    unmatched = []\n",
        "    natural_earth_path = \"/content/natural_earth_vector/10m_cultural/ne_10m_admin_1_states_provinces.shp\"\n",
        "    natural_earth_gdf = gpd.read_file(natural_earth_path)\n",
        "    # Normalize names for matching\n",
        "    natural_earth_gdf[\"name_norm\"] = natural_earth_gdf[\"name\"].str.lower().fillna(\"\")\n",
        "    natural_earth_gdf[\"admin_norm\"] = natural_earth_gdf[\"admin\"].str.lower().fillna(\"\")  # Country name\n",
        "\n",
        "    all_bounds = []  # To collect bounds of all added geometries\n",
        "\n",
        "    for loc in locations:\n",
        "        canonical = get_canonical_name(loc)\n",
        "        loc_norm = normalize_location_name(canonical)\n",
        "\n",
        "        # 1. Try match in EEZ\n",
        "        # Combine and normalize all possible name values from EEZ shapefile\n",
        "        eez_name_cols = [\"SOVEREIGN1\", \"SOVEREIGN2\", \"SOVEREIGN3\", \"TERRITORY1\", \"TERRITORY2\", \"TERRITORY3\"]\n",
        "        eez_names = set()\n",
        "\n",
        "        for col in eez_name_cols:\n",
        "            if col in eez_gdf.columns:\n",
        "                eez_names.update(eez_gdf[col].dropna().str.lower())\n",
        "\n",
        "        eez_names = list(eez_names)\n",
        "        # Try fuzzy match instead of exact match\n",
        "        best_match = fuzzy_match(loc_norm, eez_names, threshold=90)\n",
        "\n",
        "        if best_match:\n",
        "            matched_rows = eez_gdf[\n",
        "                eez_gdf[eez_name_cols].apply(lambda row: best_match in [str(val).lower() for val in row.values], axis=1)\n",
        "            ]\n",
        "        else:\n",
        "            matched_rows = gpd.GeoDataFrame()  # No match\n",
        "\n",
        "        if not matched_rows.empty:\n",
        "            for _, row in matched_rows.iterrows():\n",
        "                if not row.geometry.is_empty:\n",
        "                    folium.GeoJson(\n",
        "                        data=mapping(row.geometry),\n",
        "                        style_function=lambda x: {\n",
        "                            \"fillColor\": \"#ff9f00\",\n",
        "                            \"color\": \"#b36000\",\n",
        "                            \"weight\": 1,\n",
        "                            \"fillOpacity\": 0.5,\n",
        "                        },\n",
        "                        tooltip=folium.Tooltip(loc)\n",
        "                    ).add_to(m)\n",
        "                    # Collect bounds\n",
        "                    bounds = row.geometry.bounds  # (minx, miny, maxx, maxy)\n",
        "                    all_bounds.append([[bounds[1], bounds[0]], [bounds[3], bounds[2]]])\n",
        "            print(f\"Added EEZ polygons for '{loc}' (matched in shapefile).\")\n",
        "            continue\n",
        "\n",
        "        # 2. Try match in Natural Earth states/provinces\n",
        "        matched_states = gpd.GeoDataFrame()  # Initialize as empty\n",
        "\n",
        "        possible_names = natural_earth_gdf[\"name_norm\"].tolist() + natural_earth_gdf[\"admin_norm\"].tolist()\n",
        "        best_match = fuzzy_match(loc_norm, possible_names)\n",
        "\n",
        "        if best_match:\n",
        "            matched_states = natural_earth_gdf[\n",
        "                (natural_earth_gdf[\"name_norm\"] == best_match) |\n",
        "                (natural_earth_gdf[\"admin_norm\"] == best_match)\n",
        "            ]\n",
        "\n",
        "        if not matched_states.empty:\n",
        "            for _, row in matched_states.iterrows():\n",
        "                if not row.geometry.is_empty:\n",
        "                    folium.GeoJson(\n",
        "                        data=mapping(row.geometry),\n",
        "                        style_function=lambda x: {\n",
        "                            \"fillColor\": \"#a1d99b\",\n",
        "                            \"color\": \"#31a354\",\n",
        "                            \"weight\": 1,\n",
        "                            \"fillOpacity\": 0.4,\n",
        "                        },\n",
        "                        tooltip=folium.Tooltip(loc)\n",
        "                    ).add_to(m)\n",
        "                    # Collect bounds\n",
        "                    bounds = row.geometry.bounds  # (minx, miny, maxx, maxy)\n",
        "                    all_bounds.append([[bounds[1], bounds[0]], [bounds[3], bounds[2]]])\n",
        "            print(f\"Added Natural Earth polygons for '{loc}' (matched in admin 1 shapefile).\")\n",
        "            continue\n",
        "\n",
        "\n",
        "        # 3. Marine zones fallback\n",
        "        marine_poly = marine_zones_polygons.get(loc_norm, None)\n",
        "        if marine_poly:\n",
        "            folium.GeoJson(\n",
        "                data=mapping(marine_poly),\n",
        "                style_function=lambda x: {\n",
        "                    \"fillColor\": \"#5a9bd4\",\n",
        "                    \"color\": \"#1f4e79\",\n",
        "                    \"weight\": 1.5,\n",
        "                    \"fillOpacity\": 0.3,\n",
        "                },\n",
        "                tooltip=folium.Tooltip(loc)\n",
        "            ).add_to(m)\n",
        "            # Collect bounds\n",
        "            bounds = marine_poly.bounds  # (minx, miny, maxx, maxy)\n",
        "            all_bounds.append([[bounds[1], bounds[0]], [bounds[3], bounds[2]]])\n",
        "            print(f\"Added polygon for marine region '{loc}' (from predefined zones).\")\n",
        "            continue\n",
        "\n",
        "        # 4. No match\n",
        "        print(f\"Location '{loc}' not found in EEZ, Natural Earth, or marine zones.\")\n",
        "        unmatched.append(loc)\n",
        "\n",
        "    # After adding all geometries, fit the map to the bounds\n",
        "    if all_bounds:\n",
        "        # Find combined bounds\n",
        "        lat_min = min(b[0][0] for b in all_bounds)\n",
        "        lon_min = min(b[0][1] for b in all_bounds)\n",
        "        lat_max = max(b[1][0] for b in all_bounds)\n",
        "        lon_max = max(b[1][1] for b in all_bounds)\n",
        "        m.fit_bounds([[lat_min, lon_min], [lat_max, lon_max]])\n",
        "    else:\n",
        "        # Default view if no bounds to fit\n",
        "        m.location = [0, 0]\n",
        "        m.zoom_start = 2\n",
        "\n",
        "    title_html = f\"<h3 align='center' style='font-size:16px'><b>Distribution Map for {scientific_name}</b></h3>\"\n",
        "    m.get_root().html.add_child(folium.Element(title_html))\n",
        "\n",
        "    m.save(map_filename)\n",
        "    print(f\"Map saved to: {map_filename}\")\n",
        "\n",
        "    if unmatched:\n",
        "        print(\"\\nUnmatched locations:\")\n",
        "        for u in unmatched:\n",
        "            print(f\" - {u}\")"
      ],
      "metadata": {
        "id": "mjAkFiV8mo-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(\"/content/natural_earth_vector.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"natural_earth_vector\")  # extract to a folder\n"
      ],
      "metadata": {
        "id": "o4J-bQk_vMK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, prediction in enumerate(parsed_responses):\n",
        "    common_name = prediction.get(\"common_name\", \"\")\n",
        "    scientific_name = prediction.get(\"scientific_name\", \"\")\n",
        "    locations = prediction.get(\"locations_found_in\", [])\n",
        "    print(f\"\\nPrediction {i + 1}:\")\n",
        "    print(f\"  Common Name: {common_name}\")\n",
        "    print(f\"  Scientific Name: {scientific_name}\")\n",
        "    print(f\"  Locations: {locations}\")\n",
        "    # Generate distribution map\n",
        "    map_filename = f\"distribution_map_{i+1}_{scientific_name.replace(' ', '_')}.html\"\n",
        "    create_distribution_map(locations, scientific_name, map_filename)\n",
        "    print(f\" Distribution map saved: {map_filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyQRPwZ7uvAr",
        "outputId": "9e98dd50-d10d-4127-d478-6d9a367883e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prediction 1:\n",
            "  Common Name: Weedy Scorpionfish\n",
            "  Scientific Name: Rhinopias frondosa\n",
            "  Locations: ['Indo-Pacific Ocean', 'Indonesia', 'Philippines', 'Australia', 'Japan']\n",
            "Added polygon for marine region 'Indo-Pacific Ocean' (from predefined zones).\n",
            "Added EEZ polygons for 'Indonesia' (matched in shapefile).\n",
            "Added EEZ polygons for 'Philippines' (matched in shapefile).\n",
            "Added EEZ polygons for 'Australia' (matched in shapefile).\n",
            "Added EEZ polygons for 'Japan' (matched in shapefile).\n",
            "Map saved to: distribution_map_1_Rhinopias_frondosa.html\n",
            " Distribution map saved: distribution_map_1_Rhinopias_frondosa.html\n",
            "\n",
            "Prediction 2:\n",
            "  Common Name: Lacy Scorpionfish\n",
            "  Scientific Name: Rhinopias aphanes\n",
            "  Locations: ['Indo-Pacific Ocean', 'Great Barrier Reef', 'Papua New Guinea', 'Indonesia']\n",
            "Added polygon for marine region 'Indo-Pacific Ocean' (from predefined zones).\n",
            "Added Natural Earth polygons for 'Great Barrier Reef' (matched in admin 1 shapefile).\n",
            "Added EEZ polygons for 'Papua New Guinea' (matched in shapefile).\n",
            "Added EEZ polygons for 'Indonesia' (matched in shapefile).\n",
            "Map saved to: distribution_map_2_Rhinopias_aphanes.html\n",
            " Distribution map saved: distribution_map_2_Rhinopias_aphanes.html\n"
          ]
        }
      ]
    }
  ]
}